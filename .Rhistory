## implement fixes
texts(clean1) <- stri_replace_all_fixed(texts(clean1), "U. S.", "US")
#drop anything that's very long (95th percentile and above)
temp_lengths <- stri_length(texts(clean1))
clean2 <- corpus_subset(clean1, temp_lengths <quantile(temp_lengths, prob = .95))
scotus_lengths<-ntoken(clean2, removePunct = T)
clean3 <- corpus_trimsentences(clean2, min_length = 4)
scotus_year <- clean3$documents$Year
scotus_stat <- textstat_readability(clean3$documents$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
scotus_fre_df <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$Flesch)
scotus_fre_sent <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$meanSentenceLength)
scotus_fre_word <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$meanWordSyllables)
##too many to plot well, and very unbalanced (way more modern ones)--try for a more balanced sample
years<-unique(scotus_fre_df$year)
indices<-vector()
for(i in 1:length(years)){
set<-which(scotus_fre_df[,"year"]==years[i])
num<-min(length(set), 30)
samp<-sample(set, num, replace = FALSE)
indices<-c(indices, samp)
}
balanced_scotus_fre_df<-scotus_fre_df[indices,]
balanced_scotus_fre_sent<-scotus_fre_sent[indices,]
balanced_scotus_fre_word<-scotus_fre_word[indices,]
require(reshape2)
###Congress
load("C:/Users/kevin/Desktop/data_corpus_CR.rdata")
#drop the ones that are by the speaker
speaker <- which( (data_corpus_CR$documents$name)=="Speaker" )
clean1 <- data_corpus_CR$documents[-speaker, ]
#drop anything that's very long (90th percentile and above)
quant95 <- which( nchar(clean1$texts) >
quantile( nchar(clean1$texts), prob=.95) )
clean2 <- clean1[-quant95, ]
##sync up year format
clean2$year[clean2$year==95]<-1995
clean2$year[clean2$year==96]<-1996
clean2$year[clean2$year==97]<-1997
clean2$year[clean2$year==98]<-1998
clean2$year[clean2$year==99]<-1999
clean2$year[clean2$year==0]<-2000
clean2$year[clean2$year==1]<-2001
clean2$year[clean2$year==2]<-2002
clean2$year[clean2$year==3]<-2003
clean2$year[clean2$year==4]<-2004
clean2$year[clean2$year==5]<-2005
clean2$year[clean2$year==6]<-2006
clean2$year[clean2$year==7]<-2007
clean2$year[clean2$year==8]<-2008
##take a very small sample--can't run readability on the whole thing
sample<-sample(seq(1,length(clean2$texts)), 10000, replace = F  )
sample_data<-clean2[sample,]
congress_year <- sample_data$year
congress_stat <- textstat_readability(sample_data$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
congress_fre_df <- data.frame("year" = congress_year, "congress_stat" = congress_stat$Flesch)
congress_fre_sent <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanSentenceLength)
congress_fre_word <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanWordSyllables)
##melt together for plotting--Figure 3
require(reshape2)
df_US<-melt(list(SOTU=SOTU_fre_df, SCOTUS=balanced_scotus_fre_df, Congress = congress_fre_df,
ExecOrders = eo_fre_df), id.vars="year")
require(ggplot2)
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(-20, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Flesch Reading Ease Score") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1995, y = 45, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1800, y = 20, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 50, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1970, y = 0, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 5
df_US_sent<-melt(list(SOTU=SOTU_fre_sent, SCOTUS=balanced_scotus_fre_sent, Congress = congress_fre_sent,
ExecOrders = eo_fre_sent), id.vars="year")
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US_sent,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(15, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Sentence Length") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1960, y = 17, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1910, y = 25, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 30, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1960, y = 50, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 7
df_US_word<-melt(list(SOTU=SOTU_fre_word, SCOTUS=balanced_scotus_fre_word, Congress = congress_fre_word,
ExecOrders = eo_fre_word), id.vars="year")
p <- ggplot(data = df_US_word,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(1.4, 1.85)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Number of Syllables per Word") + theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "SOTU", x = 1800, y = 1.73, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 1.47, size = 6, colour = "black") +
annotate("text", label = "Con. Speech", x = 1998, y = 1.61, size = 6, colour = "black")+
annotate("text", label = "Executive Orders", x = 1950, y = 1.82, size = 6, colour = "black")
print(p)
##Figures 4, 6 and 8: Comparative data
data("data_corpus_SOTU")
SOTU_stat <- textstat_readability(data_corpus_SOTU, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_fre_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$Flesch)
SOTU_fre_sent <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanSentenceLength)
SOTU_fre_word <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanWordSyllables)
########nobel
#load("data_text/NobelLitePresentations/data_corpus_nobel.rdata")
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
docvars(data_corpus_nobel, "Date")
docvars(data_corpus_nobel)
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "year"))
nobel_year <- (docvars(data_corpus_nobel, "year"))
install.packages("randomizr", repos="http://r.declaredesign.org")
library("randomizr", lib.loc="~/R/win-library/3.4")
remove.packages("randomizr", lib="~/R/win-library/3.4")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr", repos = "http://r.declaredesign.org")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr")
library("randomizr", lib.loc="~/R/win-library/3.4")
library(DeclareDesign)
library(quanteda)
library(sophistication)
data("data_corpus_Crimson")
library(dplyr)
require(stringr)
require(data.table)
##read in
load("C:/Users/kevin/Documents/GitHub/sophistication-papers/analysis_article/output/fitted_BT_model.Rdata")
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
data("data_corpus_SOTU")
library(quanteda)
install.packages("quanteda")
install.packages("quanteda")
library(quanteda)
data_corpus_SOTU
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
library(sophistication)
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
library(quanteda.quanteda)
library(quanteda.corpora)
install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora", force = TRUE)
install.packages("digest")
library(readtext)
xx<-readtext(xx.txt)
xx<-readtext("xx.txt")
xx<-readtext(file ="xx.txt")
?readtext
xx<-readtext(file ="xx")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv", stringsAsFactors = F)
mturk<-read.csv("C:/Users/kevin/desktop/DLS.csv", stringsAsFactors = F)
library(tidyverse)
library(lme4)
mydata$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1
mturk$Race[mturk$Race |= "White" &  mturk$Race |= "Black or African American" & mturk$Race |= "Asian"]<-"Other"
mturk$Race[mturk$Race != "White" &  mturk$Race != "Black or African American" & mturk$Race != "Asian"]<-"Other"
mturk$Race
mturk$Race[mturk$Race == "White" &  mturk$Hispanic == "Yes"]<-"Hispanic"
mturk$Race
mturk$Overall..how.confident.do.you.feel.using.computers..smartphones..or.other.electronic.devices.to.do.the.things.you.need.to.do.online..
mturk[38,]
mturk$Race
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("Rcpp")
install.packages("Rcpp")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("digest")
install.packages("digest")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("rlang")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("testthat")
install.packages("testthat")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("backports")
install.packages("backports")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("processx")
install.packages("processx")
install.packages("ps")
install.packages("ps")
install.packages("ISOcodes")
install.packages("usethis")
install.packages("curl")
install.packages("fs")
install.packages("gh")
install.packages("git2r")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("quanteda")
devtools::install_github("kbenoit/sophistication", force=T)
library(quanteda)
devtools::install_github("kbenoit/sophistication", force=T)
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("ellipsis")
install.packages("ellipsis")
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("glue")
install.packages("glue")
install.packages("vctrs")
devtools::install_github("benjaminguinaudeau/tiktokr")
library(tiktokr)
library(reticulate)
use_python(py_config()$python)
Y
yes
use_python(py_config()$python)
install_tiktokr()
init_tiktokr()
trends <- get_trending(200)
user <- get_username("willsmith")
get_username("willsmith")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
dev.new()
plot(presCluster)
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
txt
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan", ],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
summary(presDfm)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
presDfm
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("glue")
install.packages("glue")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("rlang")
install.packages("rlang")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("processx")
install.packages("processx")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("fansi")
install.packages("fansi")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("ps")
install.packages("ps")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
library(RBERT)
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased"
)
text_to_process <- c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse.",
"An impulse is like a push.",
"Impulse is force times time.")
text_to_process2 <- list(c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse."),
c("An impulse is like a push.",
"Impulse is force times time."))
BERT_feats <- extract_features(
examples = text_to_process2,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
output_vector1 <- BERT_feats$output %>%
dplyr::filter(
sequence_index == 1,
token == "[CLS]",
layer_index == 12
) %>%
dplyr::select(dplyr::starts_with("V")) %>%
unlist()
output_vector1
install_tensorflow()
tensorflow::install_tensorflow(version = "1.13.1")
text_to_process <- c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse.",
"An impulse is like a push.",
"Impulse is force times time.")
text_to_process2 <- list(c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse."),
c("An impulse is like a push.",
"Impulse is force times time."))
BERT_feats <- extract_features(
examples = text_to_process2,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
devtools::install_github("rstudio/reticulate")
1+1 #addition
x <- c(1, 3, 4, 5) #this means you "assign" c(1, 3, 4, 5) to an object called x
1+1
y <- c(2, 6, 8, 10)
plot(x, y)
dev.new(width=10, height=10)
plot(x, y)
10 / 2
sqrt(9
sqrt(9)
sqrt(100) + sqrt(9)
exp(1)
2^3
x <- 5
y <- 10
x * y
z <- x * y
x <- x + 1
2 == 2
c(1, 2, 3) == 2
1 != 2 # not equal to
2 < 2 # less than
2 <= 2 # less than or equal to
2 > 2 # greater than
2 >= 2 # greater than or equal to
(2 < 2)
(2 < 2) | (2 <= 2) # or
(2 < 2) & (2 <= 2) # and
x <- c(TRUE, TRUE, FALSE)
x * 2
sum(x)
0 / 0 # NaN
1 / 0 # Inf
x <- c(1, NA, 0)
class("hello world!")
is.numeric("hello world!")
is.character("hello world")
class(c(1, NA, 0))
is.numeric(c(1, NA, 0))
str(as.factor(c("Blue", "Blue", "Red")))
?str
c("Blue", "Blue", "Red")
as.factor(c("Blue", "Blue", "Red"))
print(z)
z
student_names <- c("Bill", "Jane", "Sarah", "Fred", "Paul")
math_scores <- c(80, 75, 91, 67, 56)
verbal_scores <- c(72, 90, 99, 60, 68)
nums1 <- 1:100
?seq
nums2 <- seq(-10, 100, by=5) # -10, -5, 0, ..., 100
nums3 <- seq(-10, 100, length.out=467) # 467 equally spaced numbers between -10 and 100
mean(math_scores)
math_scores - verbal_scores
min(math_scores - verbal_scores)
summary(verbal_scores)
plot(math_scores, verbal_scores)
text(math_scores, verbal_scores, student_names)
math_scores[3]
math_scores[1:3]
math_scores[-(4:5)]
verbal_scores >= 90
which(verbal_scores >= 90)
math_scores[which(verbal_scores >= 90)]
math_scores[3] <- 92
math_scores
students <- data.frame(student_names, math_scores, verbal_scores)
students
summary(students)
students <- data.frame(students, final_scores = (math_scores + verbal_scores) / 2)
students
list1 <- list(some_numbers = 1:10, some_letters = c("a", "b", "c"))
list1
list1$some_numbers
getwd()
setwd(C:\Users\kevin\Documents\GitHub\TAD21)
setwd("C:\Users\kevin\Documents\GitHub\TAD21")
setwd("C:/Users/kevin/Documents/GitHub/TAD21")
install.packages("rstudioapi")
install.packages("rstudioapi")
library(rstudioapi)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
getwd()
hony <- read.csv("humansofnewyork.csv", stringsAsFactors=FALSE) # read in data, character vectors treated as character vectors (not factors)
str(hony) # examine its structure
hony$type
table(hony$type) # type of facebook post
sum(hony$type == "status")
total.likes <- sum(hony$likes_count)
total.comm <- sum(hony$comments_count)
total.shares <- sum(hony$shares_count)
total.likes + total.comm + total.shares # wow!
max(hony$shares_count) # maximum num shares
top.post <- which.max(hony$shares_count)
top.post
hony$message[top.post]
head(hony$created_time) # in order
tail(hony$created_time) # in order
first.photo <- min(which(hony$type == "photo"))
first.photo
hony$created_time[first.photo] # October 1, 2011
which(hony$type == "photo")
max.likes <- max(hony$likes_count) # likes on most popular page
sum(hony$likes_count) - max.likes
hony$likes_count > 1000000
sum(hony$likes_count > 1000000)
year <- substr(hony$created_time, 1, 4) # extracts year from date created variable
year
unique_years<-unique(year)
unique_years
unique_years<-as.numeric(unique_years)
unique_years
years<-numeric()
years
sums<-numeric()
years<-numeric()
sums<-numeric()
for(i in 1:length(unique_years)){
years[i]<-unique_years[i]
sums[i]<-sum(hony$shares_count[year == unique_years[i]])
}
years
sums
